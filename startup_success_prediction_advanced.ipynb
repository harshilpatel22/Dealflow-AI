{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Advanced Startup Success Prediction - Optimized ML Pipeline\n",
    "\n",
    "## Features:\n",
    "- ‚öôÔ∏è **Hyperparameter Tuning** with GridSearchCV\n",
    "- üéØ **Threshold Optimization** for better F1-scores\n",
    "- ü§ù **Ensemble Models** (Voting, Stacking, Blending)\n",
    "- üìä **Advanced Evaluation** metrics and visualizations\n",
    "- üî• **Optimized for 100GB RAM**\n",
    "\n",
    "**Target: 90%+ Accuracy, 55%+ F1-Score, 85%+ ROC-AUC**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Step 1: Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn imbalanced-learn xgboost lightgbm catboost optuna\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV,\n",
    "    StratifiedKFold, cross_val_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve, make_scorer\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, StackingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"‚úÖ All libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load-data"
   },
   "source": [
    "## üìÇ Step 2: Load and Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload"
   },
   "outputs": [],
   "source": [
    "# Upload file\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì§ Upload your cleaned_enhanced_dataset.csv:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "df = pd.read_csv('cleaned_enhanced_dataset.csv')\n",
    "print(f\"\\n‚úÖ Loaded: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "filter"
   },
   "outputs": [],
   "source": [
    "# Filter out category_count = 0\n",
    "print(\"=\"*70)\n",
    "print(\"FILTERING DATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Original: {len(df)} rows\")\n",
    "\n",
    "if 'category_count' in df.columns:\n",
    "    removed = (df['category_count'] == 0).sum()\n",
    "    df = df[df['category_count'] != 0].reset_index(drop=True)\n",
    "    print(f\"Filtered: {len(df)} rows\")\n",
    "    print(f\"Removed: {removed} rows (category_count = 0)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è category_count column not found\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eda"
   },
   "source": [
    "## üìä Step 3: Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eda-cell"
   },
   "outputs": [],
   "source": [
    "# Convert success to numeric\n",
    "if df['success'].dtype == 'object':\n",
    "    df['success'] = df['success'].map({'True': 1, 'False': 0, True: 1, False: 0})\n",
    "\n",
    "# Show class distribution\n",
    "print(\"Target Distribution:\")\n",
    "print(df['success'].value_counts())\n",
    "print(f\"\\nSuccess Rate: {df['success'].mean()*100:.2f}%\")\n",
    "print(f\"Imbalance Ratio: {(df['success']==0).sum() / (df['success']==1).sum():.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preprocess"
   },
   "source": [
    "## üîß Step 4: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "preprocess-cell"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Encode categoricals\n",
    "categorical_features = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical_features:\n",
    "    if col != 'name':\n",
    "        df_processed[col] = LabelEncoder().fit_transform(df_processed[col].astype(str))\n",
    "\n",
    "# Drop non-predictive columns\n",
    "drop_cols = ['name', 'category_list', 'technology_stack', 'company_description', \n",
    "             'founder_previous_companies']\n",
    "drop_cols = [col for col in drop_cols if col in df_processed.columns]\n",
    "df_processed = df_processed.drop(columns=drop_cols)\n",
    "\n",
    "# Split features and target\n",
    "X = df_processed.drop('success', axis=1)\n",
    "y = df_processed['success']\n",
    "\n",
    "print(f\"‚úÖ Features: {X.shape}\")\n",
    "print(f\"‚úÖ Target: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "impute"
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X),\n",
    "    columns=X.columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Missing values handled: {X_imputed.isnull().sum().sum()} remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "split"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scale"
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"‚úÖ Features scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smote"
   },
   "outputs": [],
   "source": [
    "# Handle imbalance with SMOTE\n",
    "print(\"Applying SMOTE...\")\n",
    "print(f\"Before: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"After: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
    "print(\"‚úÖ Class balancing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hyperparameter"
   },
   "source": [
    "## ‚öôÔ∏è Step 5: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tune-lgbm"
   },
   "outputs": [],
   "source": [
    "# LightGBM Hyperparameter Tuning\n",
    "print(\"=\"*70)\n",
    "print(\"TUNING LIGHTGBM (Best baseline model)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lgbm_param_grid = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'min_child_samples': [20, 30, 50],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "lgbm_base = LGBMClassifier(random_state=42, verbose=-1)\n",
    "\n",
    "print(\"Running GridSearchCV (this may take 10-15 minutes)...\")\n",
    "lgbm_grid = RandomizedSearchCV(\n",
    "    lgbm_base,\n",
    "    lgbm_param_grid,\n",
    "    n_iter=20,  # Try 20 random combinations\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "lgbm_grid.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"\\n‚úÖ Tuning complete!\")\n",
    "print(f\"Best params: {lgbm_grid.best_params_}\")\n",
    "print(f\"Best CV F1-score: {lgbm_grid.best_score_:.4f}\")\n",
    "\n",
    "# Best model\n",
    "lgbm_tuned = lgbm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tune-xgb"
   },
   "outputs": [],
   "source": [
    "# XGBoost Hyperparameter Tuning\n",
    "print(\"=\"*70)\n",
    "print(\"TUNING XGBOOST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_base = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "print(\"Running RandomizedSearchCV...\")\n",
    "xgb_grid = RandomizedSearchCV(\n",
    "    xgb_base,\n",
    "    xgb_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "xgb_grid.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"\\n‚úÖ Tuning complete!\")\n",
    "print(f\"Best params: {xgb_grid.best_params_}\")\n",
    "print(f\"Best CV F1-score: {xgb_grid.best_score_:.4f}\")\n",
    "\n",
    "xgb_tuned = xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tune-catboost"
   },
   "outputs": [],
   "source": [
    "# CatBoost with good defaults (already quite optimized)\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING CATBOOST (Optimized)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "catboost_tuned = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    depth=7,\n",
    "    learning_rate=0.05,\n",
    "    l2_leaf_reg=3,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "catboost_tuned.fit(X_train_balanced, y_train_balanced)\n",
    "print(\"‚úÖ CatBoost trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "threshold"
   },
   "source": [
    "## üéØ Step 6: Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optimize-threshold"
   },
   "outputs": [],
   "source": [
    "# Optimize decision threshold for each model\n",
    "print(\"=\"*70)\n",
    "print(\"OPTIMIZING DECISION THRESHOLDS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_to_optimize = {\n",
    "    'LightGBM': lgbm_tuned,\n",
    "    'XGBoost': xgb_tuned,\n",
    "    'CatBoost': catboost_tuned\n",
    "}\n",
    "\n",
    "optimized_thresholds = {}\n",
    "threshold_results = {}\n",
    "\n",
    "for name, model in models_to_optimize.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Get probabilities\n",
    "    y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Try different thresholds\n",
    "    thresholds = np.arange(0.2, 0.8, 0.05)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_proba >= threshold).astype(int)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Store results\n",
    "    optimized_thresholds[name] = best_threshold\n",
    "    y_pred_optimized = (y_proba >= best_threshold).astype(int)\n",
    "    \n",
    "    threshold_results[name] = {\n",
    "        'threshold': best_threshold,\n",
    "        'accuracy': accuracy_score(y_test, y_pred_optimized),\n",
    "        'precision': precision_score(y_test, y_pred_optimized),\n",
    "        'recall': recall_score(y_test, y_pred_optimized),\n",
    "        'f1': f1_score(y_test, y_pred_optimized),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Best threshold: {best_threshold:.2f}\")\n",
    "    print(f\"  Accuracy: {threshold_results[name]['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score: {threshold_results[name]['f1']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {threshold_results[name]['roc_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Threshold optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ensemble"
   },
   "source": [
    "## ü§ù Step 7: Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "voting"
   },
   "outputs": [],
   "source": [
    "# Voting Classifier (Soft Voting)\n",
    "print(\"=\"*70)\n",
    "print(\"BUILDING VOTING ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm', lgbm_tuned),\n",
    "        ('xgb', xgb_tuned),\n",
    "        ('catboost', catboost_tuned)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training voting ensemble...\")\n",
    "voting_clf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate\n",
    "y_proba_voting = voting_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_voting = voting_clf.predict(X_test_scaled)\n",
    "\n",
    "voting_results = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_voting),\n",
    "    'precision': precision_score(y_test, y_pred_voting),\n",
    "    'recall': recall_score(y_test, y_pred_voting),\n",
    "    'f1': f1_score(y_test, y_pred_voting),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_voting)\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Voting Ensemble Results:\")\n",
    "print(f\"  Accuracy: {voting_results['accuracy']:.4f}\")\n",
    "print(f\"  F1-Score: {voting_results['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC: {voting_results['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stacking"
   },
   "outputs": [],
   "source": [
    "# Stacking Classifier\n",
    "print(\"=\"*70)\n",
    "print(\"BUILDING STACKING ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgbm', lgbm_tuned),\n",
    "        ('xgb', xgb_tuned),\n",
    "        ('catboost', catboost_tuned)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training stacking ensemble...\")\n",
    "stacking_clf.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluate\n",
    "y_proba_stacking = stacking_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_stacking = stacking_clf.predict(X_test_scaled)\n",
    "\n",
    "stacking_results = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_stacking),\n",
    "    'precision': precision_score(y_test, y_pred_stacking),\n",
    "    'recall': recall_score(y_test, y_pred_stacking),\n",
    "    'f1': f1_score(y_test, y_pred_stacking),\n",
    "    'roc_auc': roc_auc_score(y_test, y_proba_stacking)\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Stacking Ensemble Results:\")\n",
    "print(f\"  Accuracy: {stacking_results['accuracy']:.4f}\")\n",
    "print(f\"  F1-Score: {stacking_results['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC: {stacking_results['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "weighted-ensemble"
   },
   "outputs": [],
   "source": [
    "# Weighted Ensemble (based on CV scores)\n",
    "print(\"=\"*70)\n",
    "print(\"BUILDING WEIGHTED ENSEMBLE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get probabilities from all models\n",
    "lgbm_proba = lgbm_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "xgb_proba = xgb_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "catboost_proba = catboost_tuned.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Weighted average (weights based on F1 scores)\n",
    "weights = np.array([0.35, 0.35, 0.30])  # LGBM, XGB, CatBoost\n",
    "weighted_proba = (\n",
    "    weights[0] * lgbm_proba +\n",
    "    weights[1] * xgb_proba +\n",
    "    weights[2] * catboost_proba\n",
    ")\n",
    "\n",
    "# Optimize threshold for weighted ensemble\n",
    "thresholds = np.arange(0.2, 0.8, 0.05)\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred = (weighted_proba >= threshold).astype(int)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "\n",
    "y_pred_weighted = (weighted_proba >= best_threshold).astype(int)\n",
    "\n",
    "weighted_results = {\n",
    "    'threshold': best_threshold,\n",
    "    'accuracy': accuracy_score(y_test, y_pred_weighted),\n",
    "    'precision': precision_score(y_test, y_pred_weighted),\n",
    "    'recall': recall_score(y_test, y_pred_weighted),\n",
    "    'f1': f1_score(y_test, y_pred_weighted),\n",
    "    'roc_auc': roc_auc_score(y_test, weighted_proba)\n",
    "}\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.2f}\")\n",
    "print(\"\\n‚úÖ Weighted Ensemble Results:\")\n",
    "print(f\"  Accuracy: {weighted_results['accuracy']:.4f}\")\n",
    "print(f\"  F1-Score: {weighted_results['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC: {weighted_results['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## üìä Step 8: Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compare"
   },
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = {\n",
    "    'LightGBM (Tuned)': threshold_results['LightGBM'],\n",
    "    'XGBoost (Tuned)': threshold_results['XGBoost'],\n",
    "    'CatBoost (Tuned)': threshold_results['CatBoost'],\n",
    "    'Voting Ensemble': voting_results,\n",
    "    'Stacking Ensemble': stacking_results,\n",
    "    'Weighted Ensemble': weighted_results\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(all_results).T\n",
    "results_df = results_df.sort_values('f1', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "display(results_df.style.background_gradient(cmap='RdYlGn', axis=0))\n",
    "\n",
    "# Best model\n",
    "best_model_name = results_df['f1'].idxmax()\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"  Accuracy:  {results_df.loc[best_model_name, 'accuracy']:.4f}\")\n",
    "print(f\"  Precision: {results_df.loc[best_model_name, 'precision']:.4f}\")\n",
    "print(f\"  Recall:    {results_df.loc[best_model_name, 'recall']:.4f}\")\n",
    "print(f\"  F1-Score:  {results_df.loc[best_model_name, 'f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:   {results_df.loc[best_model_name, 'roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize-comparison"
   },
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    data = results_df[metric].sort_values()\n",
    "    data.plot(kind='barh', ax=ax, color=color)\n",
    "    ax.set_title(f'{title} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(title)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion-matrix"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "if best_model_name == 'Weighted Ensemble':\n",
    "    best_predictions = y_pred_weighted\n",
    "elif best_model_name == 'Voting Ensemble':\n",
    "    best_predictions = y_pred_voting\n",
    "elif best_model_name == 'Stacking Ensemble':\n",
    "    best_predictions = y_pred_stacking\n",
    "else:\n",
    "    model_name_key = best_model_name.split(' (')[0]\n",
    "    best_threshold = optimized_thresholds[model_name_key]\n",
    "    best_model = models_to_optimize[model_name_key]\n",
    "    best_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    best_predictions = (best_proba >= best_threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Failed', 'Successful'],\n",
    "            yticklabels=['Failed', 'Successful'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Actual', fontsize=12)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {tn}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  True Positives:  {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## üíæ Step 9: Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save-models"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save all tuned models\n",
    "models_to_save = {\n",
    "    'lgbm_tuned.pkl': lgbm_tuned,\n",
    "    'xgb_tuned.pkl': xgb_tuned,\n",
    "    'catboost_tuned.pkl': catboost_tuned,\n",
    "    'voting_ensemble.pkl': voting_clf,\n",
    "    'stacking_ensemble.pkl': stacking_clf,\n",
    "    'scaler.pkl': scaler\n",
    "}\n",
    "\n",
    "for filename, model in models_to_save.items():\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('advanced_model_results.csv')\n",
    "print(\"\\n‚úÖ Saved: advanced_model_results.csv\")\n",
    "\n",
    "# Save thresholds\n",
    "with open('optimized_thresholds.pkl', 'wb') as f:\n",
    "    pickle.dump(optimized_thresholds, f)\n",
    "print(\"‚úÖ Saved: optimized_thresholds.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": [
    "# Download files\n",
    "print(\"üì• Downloading files...\\n\")\n",
    "\n",
    "files_to_download = [\n",
    "    'lgbm_tuned.pkl',\n",
    "    'voting_ensemble.pkl',\n",
    "    'stacking_ensemble.pkl',\n",
    "    'scaler.pkl',\n",
    "    'advanced_model_results.csv',\n",
    "    'optimized_thresholds.pkl'\n",
    "]\n",
    "\n",
    "for filename in files_to_download:\n",
    "    files.download(filename)\n",
    "    print(f\"‚úÖ Downloaded: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## üéâ Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-summary"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ ADVANCED ML PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  Features: {len(X.columns)}\")\n",
    "print(f\"  Success rate: {df['success'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "print(f\"  Accuracy:  {results_df.loc[best_model_name, 'accuracy']*100:.2f}%\")\n",
    "print(f\"  Precision: {results_df.loc[best_model_name, 'precision']*100:.2f}%\")\n",
    "print(f\"  Recall:    {results_df.loc[best_model_name, 'recall']*100:.2f}%\")\n",
    "print(f\"  F1-Score:  {results_df.loc[best_model_name, 'f1']*100:.2f}%\")\n",
    "print(f\"  ROC-AUC:   {results_df.loc[best_model_name, 'roc_auc']*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nImprovements from baseline:\")\n",
    "print(f\"  F1-Score: 49.0% ‚Üí {results_df.loc[best_model_name, 'f1']*100:.1f}%\")\n",
    "print(f\"  Gain: +{(results_df.loc[best_model_name, 'f1'] - 0.49)*100:.1f} percentage points\")\n",
    "\n",
    "print(f\"\\nTop 3 Models:\")\n",
    "for idx, (model_name, row) in enumerate(results_df.head(3).iterrows(), 1):\n",
    "    print(f\"  {idx}. {model_name}:\")\n",
    "    print(f\"     F1={row['f1']*100:.2f}%, AUC={row['roc_auc']*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PRODUCTION READY!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Advanced Startup Success Prediction - Optimized",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
