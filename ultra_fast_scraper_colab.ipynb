{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üöÄ Ultra-Fast Startup Scraper - Colab Edition\n",
    "## Optimized for 100+ GB RAM\n",
    "\n",
    "This notebook is specifically optimized for Google Colab's massive RAM:\n",
    "- **Unlimited caching** (no memory limits!)\n",
    "- **100+ parallel workers** for extreme speed\n",
    "- **All results in memory** for fastest access\n",
    "- **Target: 100+ companies/second** üî•\n",
    "- **Silent mode** - only shows progress bar\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages (silent mode)\n",
    "!pip install beautifulsoup4 lxml requests fake-useragent langdetect nltk tqdm\n",
    "\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload-header"
   },
   "source": [
    "## üìÅ Step 2: Upload Dataset\n",
    "\n",
    "Choose one option:\n",
    "- **Option A**: Upload file directly\n",
    "- **Option B**: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload-file"
   },
   "outputs": [],
   "source": [
    "# Option A: Upload file directly\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üì§ Upload your dataset file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "DATASET_FILE = list(uploaded.keys())[0]\n",
    "print(f\"‚úÖ File uploaded: {DATASET_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount-drive"
   },
   "outputs": [],
   "source": [
    "# Option B: Mount Google Drive (uncomment to use)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Set path to your dataset in Drive\n",
    "# DATASET_FILE = '/content/drive/MyDrive/big_startup_secsees_dataset.csv'\n",
    "# print(f\"‚úÖ Using dataset: {DATASET_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config-header"
   },
   "source": [
    "## ‚öôÔ∏è Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config"
   },
   "outputs": [],
   "source": [
    "# Processing Configuration\n",
    "START_ROW = 0\n",
    "END_ROW = 66000  # Process all rows\n",
    "\n",
    "# Performance Configuration (OPTIMIZED FOR 100+ GB RAM!)\n",
    "MAX_WORKERS = 100  # üî• EXTREME PARALLELISM!\n",
    "CHECKPOINT_INTERVAL = 1000  # Save every 1000 companies\n",
    "\n",
    "# Memory Configuration\n",
    "UNLIMITED_CACHE = True  # No cache limits!\n",
    "\n",
    "# File Configuration\n",
    "CHECKPOINT_FILE = 'scraper_checkpoint.jsonl'\n",
    "FINAL_OUTPUT_FILE = 'enhanced_dataset.csv'\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration:\")\n",
    "print(f\"   Workers: {MAX_WORKERS}\")\n",
    "print(f\"   Rows: {START_ROW} to {END_ROW}\")\n",
    "print(f\"   Memory: UNLIMITED ‚ôæÔ∏è\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "code-header"
   },
   "source": [
    "## üîß Step 4: Scraper Code (Silent Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scraper-code"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from typing import Dict, List, Optional, Any, Tuple, Set\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from dataclasses import dataclass\n",
    "import langdetect\n",
    "from fake_useragent import UserAgent\n",
    "import urllib3\n",
    "from threading import Lock\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# SILENT MODE - Disable all warnings and logs\n",
    "warnings.filterwarnings('ignore')\n",
    "urllib3.disable_warnings()\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "@dataclass\n",
    "class CompanyData:\n",
    "    name: str\n",
    "    url: str\n",
    "    category: str\n",
    "    funding: str\n",
    "    status: str\n",
    "    location: str\n",
    "    original_data: Dict[str, Any]\n",
    "\n",
    "class FastURLProcessor:\n",
    "    @staticmethod\n",
    "    def clean_and_validate_url(url: str) -> Optional[str]:\n",
    "        if not url or url == '-' or url.strip() == '':\n",
    "            return None\n",
    "        url = url.strip()\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            if '.' in url:\n",
    "                url = 'https://' + url\n",
    "            else:\n",
    "                return None\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            if not parsed.netloc or len(parsed.netloc) < 3:\n",
    "                return None\n",
    "            return url.replace(' ', '').replace('\\n', '').replace('\\r', '')\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_url_variations(url: str) -> List[str]:\n",
    "        variations = [url]\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            domain = parsed.netloc\n",
    "            path = parsed.path or ''\n",
    "            if domain.startswith('www.'):\n",
    "                non_www = domain[4:]\n",
    "                variations.append(f'https://{non_www}{path}')\n",
    "                variations.append(f'http://{non_www}{path}')\n",
    "            else:\n",
    "                variations.append(f'https://www.{domain}{path}')\n",
    "                variations.append(f'http://www.{domain}{path}')\n",
    "            if url.startswith('https://'):\n",
    "                variations.append(url.replace('https://', 'http://'))\n",
    "            elif url.startswith('http://'):\n",
    "                variations.append(url.replace('http://', 'https://'))\n",
    "        except:\n",
    "            pass\n",
    "        return variations\n",
    "\n",
    "class EnhancedContentExtractor:\n",
    "    def __init__(self):\n",
    "        self.elite_schools = {\n",
    "            'harvard', 'stanford', 'mit', 'yale', 'princeton', 'caltech',\n",
    "            'berkeley', 'oxford', 'cambridge', 'wharton', 'columbia',\n",
    "            'cornell', 'upenn', 'carnegie mellon', 'imperial', 'eth zurich'\n",
    "        }\n",
    "        self.tech_keywords = {\n",
    "            'artificial intelligence', 'machine learning', 'deep learning',\n",
    "            'blockchain', 'cryptocurrency', 'cloud computing', 'mobile app',\n",
    "            'saas platform', 'api', 'microservices', 'kubernetes', 'docker',\n",
    "            'react', 'angular', 'vue', 'python', 'java', 'golang', 'rust',\n",
    "            'data science', 'big data', 'analytics platform', 'iot',\n",
    "            'augmented reality', 'virtual reality', 'computer vision'\n",
    "        }\n",
    "        self.business_models = {\n",
    "            'software as a service', 'saas', 'platform as a service', 'paas',\n",
    "            'marketplace', 'e-commerce', 'subscription', 'freemium',\n",
    "            'enterprise software', 'b2b', 'b2c', 'd2c'\n",
    "        }\n",
    "\n",
    "    def extract_description(self, soup: BeautifulSoup, url: str = '') -> str:\n",
    "        descriptions = []\n",
    "        try:\n",
    "            selectors = [\n",
    "                ('meta[name=\"description\"]', 'content'),\n",
    "                ('meta[property=\"og:description\"]', 'content'),\n",
    "                ('meta[name=\"twitter:description\"]', 'content'),\n",
    "                ('.hero-description', 'text'),\n",
    "                ('.tagline', 'text'),\n",
    "                ('h1 + p', 'text'),\n",
    "            ]\n",
    "            for selector, attr_type in selectors:\n",
    "                try:\n",
    "                    elements = soup.select(selector)\n",
    "                    for elem in elements[:3]:\n",
    "                        desc = elem.get('content', '').strip() if attr_type == 'content' else elem.get_text().strip()\n",
    "                        if desc and 30 <= len(desc) <= 1000:\n",
    "                            descriptions.append(desc)\n",
    "                except:\n",
    "                    pass\n",
    "            if descriptions:\n",
    "                good = [d for d in descriptions if 80 <= len(d) <= 400]\n",
    "                return good[0] if good else max(descriptions, key=len)[:500]\n",
    "        except:\n",
    "            pass\n",
    "        return \"\"\n",
    "\n",
    "    def extract_founder_info(self, soup: BeautifulSoup, content: str) -> Dict[str, Any]:\n",
    "        info = {\n",
    "            'founder_count': 0,\n",
    "            'founder_education_quality': 'Unknown',\n",
    "            'founder_technical_background': False,\n",
    "            'founder_business_background': False,\n",
    "        }\n",
    "        try:\n",
    "            content_lower = content.lower()\n",
    "            founder_patterns = [r'\\b(?:founder|co-founder|ceo)\\b', r'\\bfounded by\\b']\n",
    "            founder_mentions = sum(len(re.findall(p, content_lower)) for p in founder_patterns)\n",
    "            info['founder_count'] = min(founder_mentions, 10)\n",
    "            if any(school in content_lower for school in self.elite_schools):\n",
    "                info['founder_education_quality'] = 'Elite'\n",
    "            elif any(w in content_lower for w in ['university', 'college', 'phd', 'mba']):\n",
    "                info['founder_education_quality'] = 'Good'\n",
    "            tech_count = sum(1 for i in ['engineer', 'developer', 'cto'] if i in content_lower)\n",
    "            info['founder_technical_background'] = tech_count >= 2\n",
    "            info['founder_business_background'] = any(i in content_lower for i in ['mba', 'consultant'])\n",
    "        except:\n",
    "            pass\n",
    "        return info\n",
    "\n",
    "    def extract_business_model(self, soup: BeautifulSoup, content: str) -> str:\n",
    "        try:\n",
    "            content_lower = content.lower()\n",
    "            for model in self.business_models:\n",
    "                if model in content_lower:\n",
    "                    if 'saas' in model:\n",
    "                        return 'SaaS'\n",
    "                    elif 'marketplace' in model:\n",
    "                        return 'Marketplace'\n",
    "                    elif 'e-commerce' in model:\n",
    "                        return 'E-Commerce'\n",
    "                    return model.title()\n",
    "        except:\n",
    "            pass\n",
    "        return 'Unknown'\n",
    "\n",
    "    def extract_technology_stack(self, soup: BeautifulSoup, content: str) -> List[str]:\n",
    "        technologies = []\n",
    "        try:\n",
    "            content_lower = content.lower()\n",
    "            for tech in self.tech_keywords:\n",
    "                if re.search(r'\\b' + re.escape(tech) + r'\\b', content_lower):\n",
    "                    technologies.append(tech)\n",
    "            return technologies[:3]\n",
    "        except:\n",
    "            pass\n",
    "        return []\n",
    "\n",
    "print(\"‚úÖ Classes loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scraper-main"
   },
   "outputs": [],
   "source": [
    "class Colab_UltraFastScraper:\n",
    "    def __init__(self, unlimited_cache=True, max_workers=100):\n",
    "        self.session = requests.Session()\n",
    "        self.unlimited_cache = unlimited_cache\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "        try:\n",
    "            self.ua = UserAgent()\n",
    "        except:\n",
    "            self.ua = None\n",
    "\n",
    "        retry_strategy = Retry(\n",
    "            total=1,\n",
    "            backoff_factor=0.2,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            allowed_methods=[\"HEAD\", \"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(\n",
    "            max_retries=retry_strategy,\n",
    "            pool_connections=200,\n",
    "            pool_maxsize=200\n",
    "        )\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "\n",
    "        self.url_processor = FastURLProcessor()\n",
    "        self.content_extractor = EnhancedContentExtractor()\n",
    "        self.failure_cache = set()\n",
    "        self.page_cache = {}\n",
    "\n",
    "    def get_page_content(self, url: str, timeout: int = 5) -> Tuple[Optional[BeautifulSoup], str, Dict[str, Any]]:\n",
    "        metadata = {'url': url, 'status_code': 0, 'success': False, 'error': None}\n",
    "        try:\n",
    "            clean_url = self.url_processor.clean_and_validate_url(url)\n",
    "            if not clean_url:\n",
    "                return None, \"\", metadata\n",
    "            if clean_url in self.failure_cache:\n",
    "                return None, \"\", metadata\n",
    "            if clean_url in self.page_cache:\n",
    "                cached_html = self.page_cache[clean_url]\n",
    "                soup = BeautifulSoup(cached_html, 'lxml')\n",
    "                for element in soup([\"script\", \"style\", \"noscript\"]):\n",
    "                    element.decompose()\n",
    "                content = soup.get_text(separator=' ', strip=True)\n",
    "                metadata['success'] = True\n",
    "                metadata['status_code'] = 200\n",
    "                return soup, content, metadata\n",
    "\n",
    "            url_variations = self.url_processor.get_url_variations(clean_url)\n",
    "            for attempt_url in url_variations:\n",
    "                try:\n",
    "                    user_agent = self.ua.random if self.ua else 'Mozilla/5.0'\n",
    "                    headers = {\n",
    "                        'User-Agent': user_agent,\n",
    "                        'Accept': 'text/html,application/xhtml+xml',\n",
    "                        'Connection': 'keep-alive',\n",
    "                    }\n",
    "                    response = self.session.get(attempt_url, headers=headers, timeout=timeout, allow_redirects=True, verify=False)\n",
    "                    if response.status_code >= 400:\n",
    "                        continue\n",
    "                    if 'text/html' not in response.headers.get('content-type', '').lower():\n",
    "                        continue\n",
    "                    soup = BeautifulSoup(response.content, 'lxml')\n",
    "                    for element in soup([\"script\", \"style\", \"noscript\"]):\n",
    "                        element.decompose()\n",
    "                    content = soup.get_text(separator=' ', strip=True)\n",
    "                    self.page_cache[clean_url] = response.content\n",
    "                    metadata['success'] = True\n",
    "                    metadata['status_code'] = response.status_code\n",
    "                    return soup, content, metadata\n",
    "                except:\n",
    "                    continue\n",
    "            self.failure_cache.add(clean_url)\n",
    "        except:\n",
    "            pass\n",
    "        return None, \"\", metadata\n",
    "\n",
    "    def scrape_company(self, company_data: CompanyData) -> Dict[str, Any]:\n",
    "        features = {\n",
    "            'name': company_data.name,\n",
    "            'homepage_url': company_data.url,\n",
    "            'category_list': company_data.category,\n",
    "            'funding_total_usd': company_data.funding,\n",
    "            'status': company_data.status,\n",
    "            'city': company_data.location,\n",
    "            'website_accessible': False,\n",
    "            'website_status_code': 0,\n",
    "            'founder_count': 0,\n",
    "            'founder_education_quality': 'Unknown',\n",
    "            'founder_technical_background': False,\n",
    "            'founder_business_background': False,\n",
    "            'business_model_clarity': 'Unknown',\n",
    "            'technology_stack': 'Unknown',\n",
    "            'technology_count': 0,\n",
    "            'company_description': '',\n",
    "            'content_length': 0,\n",
    "            'word_count': 0,\n",
    "            'detected_language': 'en',\n",
    "            'success': company_data.status.lower() in ['ipo', 'acquired'] if company_data.status else False\n",
    "        }\n",
    "        try:\n",
    "            soup, content, metadata = self.get_page_content(company_data.url)\n",
    "            if soup and metadata['success']:\n",
    "                features['website_accessible'] = True\n",
    "                features['website_status_code'] = metadata['status_code']\n",
    "                features['company_description'] = self.content_extractor.extract_description(soup, company_data.url)\n",
    "                features['business_model_clarity'] = self.content_extractor.extract_business_model(soup, content)\n",
    "                tech_stack = self.content_extractor.extract_technology_stack(soup, content)\n",
    "                features['technology_stack'] = ', '.join(tech_stack) if tech_stack else 'Unknown'\n",
    "                features['technology_count'] = len(tech_stack)\n",
    "                founder_info = self.content_extractor.extract_founder_info(soup, content)\n",
    "                features.update(founder_info)\n",
    "                features['content_length'] = len(content)\n",
    "                features['word_count'] = len(content.split())\n",
    "                try:\n",
    "                    if len(content) > 50:\n",
    "                        features['detected_language'] = langdetect.detect(content[:2000])\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "        return features\n",
    "\n",
    "print(\"‚úÖ Scraper ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run-header"
   },
   "source": [
    "## üöÄ Step 5: Run Scraper (Silent Mode - Progress Bar Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run-scraper"
   },
   "outputs": [],
   "source": [
    "def run_scraper():\n",
    "    print(\"üöÄ Starting scraper...\\n\")\n",
    "    \n",
    "    scraper = Colab_UltraFastScraper(unlimited_cache=UNLIMITED_CACHE, max_workers=MAX_WORKERS)\n",
    "\n",
    "    # Load dataset\n",
    "    companies = []\n",
    "    with open(DATASET_FILE, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        all_rows = list(reader)\n",
    "        rows_to_process = all_rows[START_ROW:END_ROW]\n",
    "        \n",
    "        for row in rows_to_process:\n",
    "            company_data = CompanyData(\n",
    "                name=row.get('name', ''),\n",
    "                url=row.get('homepage_url', ''),\n",
    "                category=row.get('category_list', ''),\n",
    "                funding=row.get('funding_total_usd', ''),\n",
    "                status=row.get('status', ''),\n",
    "                location=row.get('city', ''),\n",
    "                original_data=row\n",
    "            )\n",
    "            companies.append(company_data)\n",
    "\n",
    "    print(f\"üìä Processing {len(companies)} companies with {MAX_WORKERS} workers\\n\")\n",
    "\n",
    "    # Storage\n",
    "    all_results = []\n",
    "    results_lock = Lock()\n",
    "    \n",
    "    stats = {'accessible': 0, 'descriptions': 0, 'founders': 0}\n",
    "    stats_lock = Lock()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Progress bar only - no other output!\n",
    "    with tqdm(total=len(companies), desc=\"Scraping\", unit=\" companies\", ncols=100) as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            future_to_company = {executor.submit(scraper.scrape_company, c): c for c in companies}\n",
    "\n",
    "            checkpoint_buffer = []\n",
    "\n",
    "            for future in as_completed(future_to_company):\n",
    "                try:\n",
    "                    result = future.result(timeout=15)\n",
    "                    \n",
    "                    with results_lock:\n",
    "                        all_results.append(result)\n",
    "                        checkpoint_buffer.append(result)\n",
    "                    \n",
    "                    with stats_lock:\n",
    "                        if result.get('website_accessible'):\n",
    "                            stats['accessible'] += 1\n",
    "                        if result.get('company_description'):\n",
    "                            stats['descriptions'] += 1\n",
    "                        if result.get('founder_count', 0) > 0:\n",
    "                            stats['founders'] += 1\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "\n",
    "                    # Checkpoint\n",
    "                    if len(all_results) % CHECKPOINT_INTERVAL == 0:\n",
    "                        with open(CHECKPOINT_FILE, 'a', encoding='utf-8') as f:\n",
    "                            for item in checkpoint_buffer:\n",
    "                                f.write(json.dumps(item) + '\\n')\n",
    "                        checkpoint_buffer = []\n",
    "                        \n",
    "                        # Update progress bar with stats\n",
    "                        elapsed = time.time() - start_time\n",
    "                        rate = len(all_results) / elapsed\n",
    "                        pbar.set_postfix({\n",
    "                            'rate': f'{rate:.1f}/s',\n",
    "                            'ok': stats['accessible'],\n",
    "                            'desc': stats['descriptions']\n",
    "                        })\n",
    "\n",
    "                except:\n",
    "                    pbar.update(1)\n",
    "\n",
    "            # Flush buffer\n",
    "            if checkpoint_buffer:\n",
    "                with open(CHECKPOINT_FILE, 'a', encoding='utf-8') as f:\n",
    "                    for item in checkpoint_buffer:\n",
    "                        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "    # Save CSV\n",
    "    print(\"\\nüíæ Saving results...\")\n",
    "    with open(FINAL_OUTPUT_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        if all_results:\n",
    "            fieldnames = list(all_results[0].keys())\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_results)\n",
    "\n",
    "    # Final stats\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚è±Ô∏è  Time: {elapsed/60:.1f} minutes\")\n",
    "    print(f\"‚ö° Rate: {len(all_results)/elapsed:.1f} companies/second\")\n",
    "    print(f\"üìä Total: {len(all_results)} companies\")\n",
    "    print(f\"üåê Accessible: {stats['accessible']} ({stats['accessible']/len(all_results)*100:.1f}%)\")\n",
    "    print(f\"üìù Descriptions: {stats['descriptions']} ({stats['descriptions']/len(all_results)*100:.1f}%)\")\n",
    "    print(f\"üë• Founders: {stats['founders']} ({stats['founders']/len(all_results)*100:.1f}%)\")\n",
    "    print(f\"\\nüìÅ Output: {FINAL_OUTPUT_FILE}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    gc.collect()\n",
    "    return all_results\n",
    "\n",
    "# RUN IT!\n",
    "results = run_scraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-header"
   },
   "source": [
    "## üì• Step 6: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-results"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading...\")\n",
    "files.download(FINAL_OUTPUT_FILE)\n",
    "print(\"‚úÖ Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stats-header"
   },
   "source": [
    "## üìä Step 7: Quick Analysis (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-stats"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(FINAL_OUTPUT_FILE)\n",
    "\n",
    "print(\"üìä Quick Stats:\")\n",
    "print(f\"\\nTotal: {len(df)}\")\n",
    "print(f\"\\nAccessible:\")\n",
    "print(df['website_accessible'].value_counts())\n",
    "print(f\"\\nSuccess:\")\n",
    "print(df['success'].value_counts())\n",
    "print(f\"\\nTop Business Models:\")\n",
    "print(df['business_model_clarity'].value_counts().head(5))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Ultra-Fast Scraper - Silent Mode",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
